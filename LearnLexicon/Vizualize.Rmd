---
title: "Presentation Graphs"
author: "Frank Mollica"
date: "September 11, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, width = 1000, fig.align = 'center', fig.width = 16)
```

```{r preprocess, message=FALSE, warning=FALSE}
library(plyr)
library(gdata)
library(ggplot2)
library(reshape)
library(matrixStats)

exp_stats <- function(df, amount, lt) {
  posterior = df$LPrior + (amount*df$LLike)/lt
  postR = df$RPrior + (amount*df$LLike)/lt
  lse = logSumExp(posterior)
  lseR = logSumExp(postR)
  p = exp(posterior - lse)
  pR = exp(postR - lseR)
  data.frame(amount=amount, 
             Prior=c('Simplicity', 'Simplicity &\nReuse'),
             Accuracy=c(sum(p*df$ACC), sum(pR*df$ACC)), 
             Precision=c(sum(p*df$Precision),sum(pR*df$Precision)), 
             Recall=c(sum(p*df$Recall), sum(pR*df$Recall)),
             Abstract=c(sum(p*df$A), sum(pR*df$A)),
             LexAcc=c(sum(p*df$LA), sum(pR*df$LA)),
             LexPre=c(sum(p*df$LP), sum(pR*df$LP)),
             LexRec=c(sum(p*df$LR), sum(pR*df$LR))
             )
}

marg_stats <- function(df, amount, lt) {
  posterior = df$HPrior + (amount*df$HLike)/lt
  lse = logSumExp(posterior)
  p = exp(posterior - lse)
  data.frame(amount=amount, 
             Prior=c('Simplicity'),
             Accuracy=c(sum(p*df$ACC)), 
             Precision=c(sum(p*df$Precision)), 
             Abstract=c(sum(p*df$A)),
             Recall=c(sum(p*df$Recall)))
}

find_min = function(df, lim=0.99, meas='Accuracy'){
  min(df$amount[df$value >= lim & df$variable==meas])
}

```

# Learnability Proof

```{r, english}

eng = read.csv('NEW_COMBO_ENG.csv', header = F)
colnames(eng) = c('HypNo', 'Word', 'HPrior', 'HLike', 'LPrior', 'RPrior', 'LLike', 'Correct', 'Proposed', 'Truth', 'A')

eng$ACC = 0
eng$ACC[eng$Correct==eng$Proposed & eng$Proposed==eng$Truth] = 1

eng$Precision = eng$Correct / eng$Proposed
eng$Precision[is.nan(eng$Precision)] = 0
eng$Recall = eng$Correct / eng$Truth

eng$Word = trim(eng$Word)

message(paste0('Percent Words Learned: ', round(100*length(unique(eng$Word[eng$ACC==1]))/length(unique(eng$Word)),2),'%'))

la = ddply(eng, .(HypNo), summarise, LA=as.numeric(mean(ACC)==1), LP=sum(Correct)/sum(Proposed), LR=sum(Correct)/sum(Truth))
eng = merge(eng, la)

message(paste0('Percent Accurate Lexicons: ', round(100*sum(la$LA)/dim(la)[1],2), '%'))

data = NULL
for (amt in seq(0, 700, 1)) {
  k = ddply(eng, .(Word), function(Z) {exp_stats(Z, amt, 1)})
  data = rbind(data, k)
}

data=melt(data, id=c('Prior', 'Word', 'amount'))
```

## Learning Curves by Word according to Lexicon Posterior

```{r}
learnablity = subset(data, Prior=='Simplicity' & (variable %in% c('Accuracy'))) #,'Precision','Recall')))

mins = ddply(learnablity, .(Word), find_min)
mins$V1 = mins$V1 + 100
learnablity = merge(learnablity, mins)

learnablity = learnablity[learnablity$amount <= learnablity$V1,]

ggplot(learnablity, aes(x=amount, value, linetype=variable, color=variable)) +
  facet_wrap(~Word, scales='free_x', nrow=2) +
  labs(y='Posterior Accuracy', x='Number of Data Points') +
  geom_line(size=1) +
  theme_bw() +
  guides(linetype=F, color=F) +
  scale_colour_brewer(palette="Set1") +
  ggtitle('English') +
  theme(legend.title=element_blank()) 
#ggsave('~/Presentations/NRT/Figures/EnglishSimplicityRed.png', width=9, height=5)
```

## Learning Curves by Word: Simplicity vs Reuse

```{r}
learnablity = subset(data, (variable %in% c('Accuracy','Precision','Recall')))

mins = ddply(learnablity[learnablity$Prior=='Simplicity',], .(Word), find_min)
mins$V1 = mins$V1 + 100
learnablity = merge(learnablity, mins)

learnablity = learnablity[learnablity$amount <= learnablity$V1,]

ggplot(subset(learnablity, variable=='Accuracy'), aes(x=amount, value, linetype=Prior, color=Prior)) +
  facet_grid(.~Word, scales='free_x') +
  labs(y='Proportion', x='Number of Data Points') +
  geom_line(size=1) +
  theme_bw() +
  scale_colour_brewer(palette="Set1") +
  theme(legend.title=element_blank()) 
```

## Abstraction Analysis (posterior weighted)

```{r}
ab = subset(data, variable=='Abstract' & Prior=='Simplicity')
ab = ab[ab$amount <= 20,]

ggplot(ab, aes(x=amount, value, color=variable, linetype=variable)) +
  facet_grid(.~Word, scales='free_x') +
  labs(y='Probability of Abstraction', x='Number of Data Points') +
  geom_line(size=1) +
  theme_bw() +
  guides(color=F, linetype=F) +
  scale_colour_brewer(palette="Set1") +
  theme(legend.title=element_blank()) 

```

## Learning Curve by Lexicon

```{r}
learn_lex = subset(data, Prior=='Simplicity' & !(variable %in% c('Accuracy','Precision','Recall', 'Abstract')))

mins = ddply(learn_lex, .(Word), function(Z) {find_min(Z, meas='LexAcc')})
mins$V1 = mins$V1 + 100
learn_lex = merge(learn_lex, mins)

learn_lex = learn_lex[learn_lex$amount <= learn_lex$V1,]

learn_lex$variable = mapvalues(learn_lex$variable, 
                               from=c('LexAcc', 'LexPre', 'LexRec'), 
                               to=c('Accuracy', 'Precision', 'Recall'))

ggplot(subset(learn_lex, Word==unique(eng$Word)[1]), 
       aes(x=amount, value, color=variable, linetype=variable)) +
  labs(y='Proportion', x='Number of Data Points') +
  geom_line(size=1) +
  theme_bw() +
  scale_colour_brewer(palette="Set1") +
  theme(legend.title=element_blank()) 
```

## Learning Curve by Lexicon: Simplicity vs Reuse

```{r}
learn_lex = subset(data, !(variable %in% c('Accuracy','Precision','Recall', 'Abstract')))

learn_lex = merge(learn_lex, mins)

learn_lex = learn_lex[learn_lex$amount <= learn_lex$V1,]

learn_lex$variable = mapvalues(learn_lex$variable, 
                               from=c('LexAcc', 'LexPre', 'LexRec'), 
                               to=c('Accuracy', 'Precision', 'Recall'))

ggplot(subset(learn_lex, Word==unique(eng$Word)[1] & variable=='Accuracy'), 
       aes(x=amount, value, color=Prior, linetype=Prior)) +
  labs(y='Proportion', x='Number of Data Points') +
  geom_line(size=1) +
  theme_bw() +
  scale_colour_brewer(palette="Set1") +
  theme(legend.title=element_blank()) 
```

## Learning Curves per Word Marginalizing over Lexicons

```{r}
data = NULL
for (amt in seq(0, 200, 1)) {
  k = ddply(eng, .(Word), function(Z) {marg_stats(Z, amt, 1)})
  data = rbind(data, k)
}

data=melt(data, id=c('Prior', 'Word', 'amount'))

learnablity = subset(data, Prior=='Simplicity' & (variable %in% c('Accuracy','Precision','Recall')))

mins = ddply(learnablity, .(Word), find_min)
mins$V1 = mins$V1 + 15
learnablity = merge(learnablity, mins)

learnablity = learnablity[learnablity$amount <= learnablity$V1,]

ggplot(learnablity, aes(x=amount, value, linetype=variable, color=variable)) +
  facet_grid(.~Word, scales='free_x') +
  labs(y='Proportion', x='Number of Data Points') +
  geom_line(size=1) +
  theme_bw() +
  scale_colour_brewer(palette="Set1") +
  theme(legend.title=element_blank()) 

```

## Abstraction Analysis (based on hypothesis likelihood)

```{r}
ab = subset(data, variable=='Abstract' & Prior=='Simplicity')
ab = ab[ab$amount <= 20,]

ggplot(ab, aes(x=amount, value, color=variable, linetype=variable)) +
  facet_grid(.~Word, scales='free_x') +
  labs(y='Probability of Abstraction', x='Number of Data Points') +
  theme_bw() +
  geom_line(size=1) +
  guides(color=F, linetype=F) +
  scale_colour_brewer(palette="Set1") +
  theme(legend.title=element_blank()) 

```




